import torch
import time
import logging
from thop import profile
import psutil
import os


def count_parameters(model):
    """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params


def calculate_flops(model, input_shape, device):
    """è®¡ç®—æ¨¡å‹FLOPS"""
    try:
        # åˆ›å»ºéšæœºè¾“å…¥
        dummy_input = torch.randn(input_shape).to(device)
        
        # è®¡ç®—FLOPS
        flops, params = profile(model, inputs=(dummy_input, torch.tensor([0]).to(device)))
        return flops, params
    except Exception as e:
        logging.warning(f"FLOPS calculation failed: {e}")
        return 0, 0


def get_gpu_memory_usage():
    """è·å–GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        # è·å–å½“å‰GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        allocated = torch.cuda.memory_allocated() / 1024**2  # MB
        reserved = torch.cuda.memory_reserved() / 1024**2   # MB
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**2  # MB
        free_memory = total_memory - reserved
        
        return {
            'allocated': allocated,
            'reserved': reserved,
            'total': total_memory,
            'free': free_memory
        }
    else:
        return None


def measure_inference_time(model, input_shape, device, num_runs=10):
    """æµ‹é‡æ¨ç†æ—¶é—´"""
    model.eval()
    dummy_input = torch.randn(input_shape).to(device)
    dummy_t = torch.tensor([0]).to(device)
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(3):
            _ = model(dummy_input, dummy_t)
    
    # æµ‹é‡æ—¶é—´
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    start_time = time.time()
    
    with torch.no_grad():
        for _ in range(num_runs):
            _ = model(dummy_input, dummy_t)
    
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    end_time = time.time()
    
    avg_time = (end_time - start_time) / num_runs * 1000  # ms
    return avg_time


def profile_model(model, input_shape, device, model_name):
    """å®Œæ•´çš„æ¨¡å‹æ€§èƒ½åˆ†æ"""
    print("=" * 60)
    print(f"Model Profiling for {model_name}")
    print("=" * 60)
    
    # 1. å‚æ•°é‡ç»Ÿè®¡
    total_params, trainable_params = count_parameters(model)
    print(f"ğŸ“Š Model Parameters:")
    print(f"  Total Parameters: {total_params:,} ({total_params/1e6:.2f}M)")
    print(f"  Trainable Parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)")
    print()
    
    # 2. GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ˆæ¨¡å‹åŠ è½½å‰ï¼‰
    memory_before = get_gpu_memory_usage()
    
    # å°†æ¨¡å‹ç§»åˆ°GPUï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
    model = model.to(device)
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    # GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ˆæ¨¡å‹åŠ è½½åï¼‰
    memory_after = get_gpu_memory_usage()
    
    if memory_before and memory_after:
        model_memory = memory_after['allocated'] - memory_before['allocated']
        print(f"ğŸ’¾ GPU Memory Usage:")
        print(f"  Before Model: {memory_before['allocated']:.1f}MB allocated, {memory_before['reserved']:.1f}MB reserved")
        print(f"  After Model:  {memory_after['allocated']:.1f}MB allocated, {memory_after['reserved']:.1f}MB reserved")
        print(f"  Model Memory: {model_memory:.1f}MB")
        print(f"  Total GPU Memory: {memory_after['total']:.1f}MB")
        print(f"  Free Memory: {memory_after['free']:.1f}MB")
        print()
    
    # 3. FLOPSè®¡ç®—
    flops, _ = calculate_flops(model, input_shape, device)
    if flops > 0:
        print(f"âš¡ Performance Metrics:")
        print(f"  FLOPs: {flops/1e9:.3f}G ({flops:,})")
    
    # 4. æ¨ç†æ—¶é—´æµ‹é‡
    try:
        inference_time = measure_inference_time(model, input_shape, device)
        print(f"  Inference Time: {inference_time:.2f}ms")
    except Exception as e:
        print(f"  Inference Time: Failed to measure ({e})")
    
    print()
    
    # 5. è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¿¡æ¯
    try:
        dummy_input = torch.randn(input_shape).to(device)
        dummy_t = torch.tensor([0]).to(device)
        with torch.no_grad():
            output = model(dummy_input, dummy_t)
            if isinstance(output, tuple):
                output_shape = output[0].shape
            else:
                output_shape = output.shape
        print(f"ğŸ“ Input/Output Shapes:")
        print(f"  Input Shape: {list(input_shape)}")
        print(f"  Output Shape: {list(output_shape)}")
    except Exception as e:
        print(f"ğŸ“ Shape Info: Failed to get output shape ({e})")
    
    print("=" * 60)
    print()
    
    # è¿”å›ç»Ÿè®¡ä¿¡æ¯ç”¨äºæ—¥å¿—è®°å½•
    results = {
        'total_params': total_params,
        'trainable_params': trainable_params,
        'total_params_M': total_params/1e6,
        'trainable_params_M': trainable_params/1e6,
        'flops': flops,
        'flops_G': flops/1e9 if flops > 0 else 0,
        'model_memory_MB': model_memory if memory_before and memory_after else 0,
        'inference_time_ms': inference_time if 'inference_time' in locals() else 0
    }
    
    return results


def log_profiling_results(results, model_name, log_file_path):
    """å°†æ€§èƒ½åˆ†æç»“æœå†™å…¥æ—¥å¿—æ–‡ä»¶"""
    try:
        with open(log_file_path, 'a', encoding='utf-8') as f:
            f.write(f"\n{'='*60}\n")
            f.write(f"Model Profiling Results for {model_name}\n")
            f.write(f"{'='*60}\n")
            f.write(f"Total Parameters: {results['total_params']:,} ({results['total_params_M']:.2f}M)\n")
            f.write(f"Trainable Parameters: {results['trainable_params']:,} ({results['trainable_params_M']:.2f}M)\n")
            f.write(f"FLOPs: {results['flops_G']:.3f}G ({results['flops']:,})\n")
            f.write(f"Model Memory: {results['model_memory_MB']:.1f}MB\n")
            f.write(f"Inference Time: {results['inference_time_ms']:.2f}ms\n")
            f.write(f"{'='*60}\n\n")
    except Exception as e:
        logging.warning(f"Failed to write profiling results to log: {e}")


def get_model_input_shape(model_name, dataset_type, image_size, batch_size=1):
    """æ ¹æ®æ¨¡å‹åç§°å’Œæ•°æ®é›†ç±»å‹ç¡®å®šè¾“å…¥å½¢çŠ¶"""
    # ç¡®å®šé€šé“æ•°
    if dataset_type in ["mripet", "mrispect"]:
        in_channels = 6  # RGB + RGB
    else:
        in_channels = 2  # Two grayscale images
    
    # å¯¹äºå°æ³¢æ¨¡å‹ï¼Œè¾“å…¥å½¢çŠ¶ä¼šæœ‰æ‰€ä¸åŒ
    if model_name in ["WTDDPM"] or model_name.startswith("D3CG"):
        # å°æ³¢æ¨¡å‹çš„è¾“å…¥å½¢çŠ¶
        if dataset_type in ["mripet", "mrispect"]:
            # RGBå›¾åƒçš„å°æ³¢ç³»æ•°ï¼šæ¯ä¸ªé€šé“4ä¸ªç³»æ•°(LL, LH, HL, HH)ï¼Œä¸¤ä¸ªå›¾åƒå…±6ä¸ªé€šé“
            wavelet_channels = 24  # 6 channels * 4 coefficients
        else:
            # ç°åº¦å›¾åƒçš„å°æ³¢ç³»æ•°ï¼šæ¯ä¸ªé€šé“4ä¸ªç³»æ•°ï¼Œä¸¤ä¸ªå›¾åƒå…±2ä¸ªé€šé“
            wavelet_channels = 8   # 2 channels * 4 coefficients
        
        # å°æ³¢å˜æ¢åçš„å°ºå¯¸æ˜¯åŸæ¥çš„ä¸€åŠ
        return (batch_size, wavelet_channels, image_size//2, image_size//2)
    else:
        # æ™®é€šæ¨¡å‹çš„è¾“å…¥å½¢çŠ¶
        return (batch_size, in_channels, image_size, image_size)